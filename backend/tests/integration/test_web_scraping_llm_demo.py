"""Demo integration test that works without API keys

This demonstrates the full pipeline:
1. Scrapes a real webpage
2. Processes with LLM (uses mock if no API key)
3. Generates document.md file
"""

import pytest
import pytest_asyncio
import asyncio
import os
from pathlib import Path
from datetime import datetime
import hashlib

from src.services.web_scraping_service import WebScrapingService
from src.services.llm_service import LLMService, LLMProvider
from src.models.document import (
    DocumentMessage,
    DocumentType,
    DocumentMetadata,
    DocumentContent,
    ProcessingStage,
    DocumentSource
)
from src.core.config import get_settings
from src.storage.storage_factory import StorageFactory, StorageType


@pytest_asyncio.fixture
async def web_scraping_service():
    """Create real WebScrapingService instance"""
    service = WebScrapingService()
    yield service
    await service.cleanup()


@pytest_asyncio.fixture
async def llm_service():
    """Create LLMService (will use mock if no API key)"""
    service = LLMService(provider=LLMProvider.ANTHROPIC)
    yield service


@pytest_asyncio.fixture
async def storage():
    """Create storage instance"""
    storage = StorageFactory.create(StorageType.FILESYSTEM)
    yield storage


class TestWebScrapingLLMDemo:
    """Demo integration tests that work without API keys"""

    @pytest.mark.asyncio
    async def test_complete_pipeline_demo(
        self,
        web_scraping_service,
        llm_service,
        storage,
        tmp_path
    ):
        """Demo the complete pipeline: scrape ‚Üí process ‚Üí generate document"""

        # Use a simple, stable webpage
        test_url = "https://httpbin.org/html"

        print(f"\n{'='*60}")
        print("üöÄ DEMO: Web Scraping + LLM Pipeline")
        print(f"{'='*60}")
        print(f"\n1Ô∏è‚É£ Scraping webpage: {test_url}")

        # Step 1: Scrape the webpage
        scraped_document = await web_scraping_service.scrape_webpage(
            url=test_url,
            use_javascript=False,
            user_id="demo_user",
            session_id="demo_session"
        )

        assert scraped_document is not None
        assert scraped_document.content.raw_text
        print(f"   ‚úÖ Scraped successfully")
        print(f"   üìÑ Title: {scraped_document.metadata.name}")
        print(f"   üìè Content size: {len(scraped_document.content.raw_text)} chars")

        # Step 2: Process with LLM operations
        print(f"\n2Ô∏è‚É£ Processing with LLM Service")

        settings = get_settings()
        has_api_key = bool(settings.llm.anthropic_api_key and
                          settings.llm.anthropic_api_key != "your-anthropic-api-key")

        if has_api_key:
            print("   üîë Using real Claude API")
        else:
            print("   üé≠ Using mock responses (no API key configured)")

        # Process document with AI operations
        operations = ["summary", "entities", "classify"]
        enhanced_document = await llm_service.process_document_with_ai(
            scraped_document,
            operations
        )

        assert enhanced_document.metadata.summary
        print(f"   ‚úÖ Generated summary: {len(enhanced_document.metadata.summary)} chars")

        assert enhanced_document.content.structured_data
        entities = enhanced_document.content.structured_data.get("entities", [])
        print(f"   ‚úÖ Extracted entities: {len(entities)} found")

        classification = enhanced_document.content.structured_data.get("classification", {})
        print(f"   ‚úÖ Classification: {classification.get('category', 'Unknown')} "
              f"(confidence: {classification.get('confidence', 0):.2f})")

        # Step 3: Generate document.md file
        print(f"\n3Ô∏è‚É£ Generating document.md file")

        output_dir = tmp_path / "output"
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / "document.md"

        # Create the document content
        doc_content = f"""# {enhanced_document.metadata.name or 'Web Document'}

**Source**: {test_url}
**Generated**: {datetime.utcnow().isoformat()}
**Document ID**: {enhanced_document.metadata.document_id}

## Summary

{enhanced_document.metadata.summary}

## Document Classification

- **Category**: {classification.get('category', 'Unknown')}
- **Confidence**: {classification.get('confidence', 0):.2f}

## Extracted Entities

"""
        # Add entities
        if entities:
            for entity in entities[:10]:
                doc_content += f"- **{entity.get('type', 'UNKNOWN')}**: {entity.get('text', '')}\n"
        else:
            doc_content += "*No entities extracted*\n"

        doc_content += f"""

## Original Content (Preview)

```
{scraped_document.content.raw_text[:500]}...
```

---

*Generated by Mavn Bench Integration Pipeline*
*Service: WebScrapingService + LLMService*
*Provider: {'Claude (Real)' if has_api_key else 'Mock (Demo Mode)'}*
"""

        # Write to file
        output_file.write_text(doc_content)
        print(f"   ‚úÖ Generated: {output_file}")

        # Step 4: Store in document storage
        print(f"\n4Ô∏è‚É£ Storing in document system")

        doc_id = enhanced_document.metadata.document_id
        await storage.store(doc_id, enhanced_document.model_dump())

        # Verify storage
        stored_doc = await storage.load(doc_id)
        assert stored_doc is not None
        print(f"   ‚úÖ Stored with ID: {doc_id}")

        # Step 5: Display results
        print(f"\n{'='*60}")
        print("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
        print(f"{'='*60}")
        print(f"üìä Results:")
        print(f"   - Webpage scraped: {test_url}")
        print(f"   - Content processed: {len(scraped_document.content.raw_text)} chars")
        print(f"   - Summary generated: {len(enhanced_document.metadata.summary)} chars")
        print(f"   - Entities found: {len(entities)}")
        print(f"   - Document saved: {output_file}")
        print(f"   - Stored in system: {doc_id}")

        if not has_api_key:
            print(f"\nüí° To use real Claude API:")
            print(f"   1. Set ANTHROPIC_API_KEY in .env file")
            print(f"   2. Re-run the test for actual AI processing")

        # Cleanup
        await storage.delete(doc_id)

        # Verify the generated file exists and has content
        assert output_file.exists()
        content = output_file.read_text()
        assert "# " in content  # Has headers
        assert test_url in content  # Contains source URL
        assert doc_id in content  # Contains document ID

        return {
            "output_file": str(output_file),
            "doc_id": doc_id,
            "content_size": len(scraped_document.content.raw_text),
            "summary_size": len(enhanced_document.metadata.summary),
            "entities_count": len(entities)
        }

    @pytest.mark.asyncio
    async def test_extract_links_and_process(
        self,
        web_scraping_service,
        llm_service
    ):
        """Demo extracting links from a page and processing them"""

        test_url = "https://httpbin.org/"

        print(f"\n{'='*60}")
        print("üîó DEMO: Link Extraction and Processing")
        print(f"{'='*60}")

        # Extract links
        print(f"\nüìé Extracting links from: {test_url}")
        links = await web_scraping_service.extract_links(test_url)

        print(f"   ‚úÖ Found {len(links)} links")

        # Show first 5 links
        for i, link in enumerate(links[:5], 1):
            print(f"   {i}. {link}")

        if len(links) > 5:
            print(f"   ... and {len(links) - 5} more")

        # Process one of the links
        if links:
            # Find a simple HTML endpoint
            html_links = [l for l in links if 'html' in l.lower()]
            if html_links:
                target_link = html_links[0]
            else:
                target_link = links[0]

            print(f"\nüìÑ Processing link: {target_link}")

            try:
                doc = await web_scraping_service.scrape_webpage(target_link)

                # Generate a quick summary
                summary = await llm_service.generate_summary(
                    doc.content.raw_text[:500],
                    max_length=50,
                    style="concise"
                )

                print(f"   ‚úÖ Processed successfully")
                print(f"   üìù Summary: {summary[:100]}...")

            except Exception as e:
                print(f"   ‚ö†Ô∏è Could not process link: {str(e)}")

        assert len(links) > 0

    @pytest.mark.asyncio
    async def test_error_handling(
        self,
        web_scraping_service,
        llm_service
    ):
        """Demo error handling in the pipeline"""

        print(f"\n{'='*60}")
        print("üõ°Ô∏è DEMO: Error Handling")
        print(f"{'='*60}")

        # Test 1: Invalid URL
        print("\n1Ô∏è‚É£ Testing invalid URL handling")
        invalid_url = "https://this-does-not-exist-999999.com"

        try:
            await web_scraping_service.scrape_webpage(invalid_url)
            assert False, "Should have raised an exception"
        except Exception as e:
            print(f"   ‚úÖ Properly handled: {type(e).__name__}")

        # Test 2: Empty content
        print("\n2Ô∏è‚É£ Testing empty content handling")
        summary = await llm_service.generate_summary("")
        assert summary == "No text provided for summarization."
        print(f"   ‚úÖ Empty content handled gracefully")

        # Test 3: Very large content
        print("\n3Ô∏è‚É£ Testing large content handling")
        large_text = "This is a test. " * 1000
        summary = await llm_service.generate_summary(large_text, max_length=50)
        assert len(summary) < len(large_text)
        print(f"   ‚úÖ Large content summarized: {len(large_text)} ‚Üí {len(summary)} chars")

        print(f"\n‚úÖ All error cases handled properly!")


# Standalone function to run the demo
async def run_demo():
    """Run the demo without pytest"""
    print("\n" + "="*70)
    print(" "*20 + "MAVN BENCH INTEGRATION DEMO")
    print("="*70)

    # Initialize services
    web_service = WebScrapingService()
    llm_service = LLMService(provider=LLMProvider.ANTHROPIC)
    storage = StorageFactory.create(StorageType.FILESYSTEM)

    # Run the demo
    test = TestWebScrapingLLMDemo()

    # Create temp directory
    from tempfile import TemporaryDirectory
    with TemporaryDirectory() as tmp_dir:
        tmp_path = Path(tmp_dir)

        result = await test.test_complete_pipeline_demo(
            web_service,
            llm_service,
            storage,
            tmp_path
        )

        print(f"\nüìä Demo Results:")
        for key, value in result.items():
            print(f"   {key}: {value}")

    # Cleanup
    await web_service.cleanup()

    print("\n" + "="*70)
    print(" "*15 + "DEMO COMPLETED SUCCESSFULLY!")
    print("="*70)


if __name__ == "__main__":
    # Allow running directly without pytest
    asyncio.run(run_demo())